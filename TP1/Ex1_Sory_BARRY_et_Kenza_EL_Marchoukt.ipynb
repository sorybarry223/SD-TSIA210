{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafPdtuncbq7"
      },
      "source": [
        "<h1><center>MNIST classification using Numpy<center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOM: EL MARCHOUK KENZA et BARRY SORY IBRAHIMA**"
      ],
      "metadata": {
        "id": "ZPLVhya1McmI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4VrCB5La5rD"
      },
      "source": [
        "## Importing Numpy and Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlKZ3Hnas7B4",
        "outputId": "7024b3d5-02a6-49e8-9593-0fa5a854b710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version 2.8.0\n",
            "Using keras version 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "print(\"Using keras version \" + str(keras.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QLz9_jbRZq"
      },
      "source": [
        "## Loading and preparing the MNIST dataset\n",
        "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG83hGyVmijn",
        "outputId": "cd8bf7be-e6e6-4484-8ff4-8b01ecedb525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the size of training set:  (60000, 28, 28) (60000,)\n",
            "the size of testing sets:  (10000, 28, 28) (10000,)\n",
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
        "from keras.datasets import mnist\n",
        "# START CODE HERE\n",
        "data = mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = data\n",
        "print(\"the size of training set: \",X_train.shape,y_train.shape)\n",
        "print(\"the size of testing sets: \", X_test.shape,y_test.shape)\n",
        "# END CODE HERE\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRPbU_Z4U6Ac"
      },
      "source": [
        "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
        "Using the pyplot package, visualize the first sample of the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "x5VAu7oW0Zu4",
        "outputId": "b9de2328-13ea-48f8-aa86-0e9aede6a9a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x288 with 18 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAADnCAYAAACnrY0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd2BUVfr/8TuZhDQIEEJCDSEkIVRBQKqgYl8EERQRVsSCgIAiqLuuu+7aVsVGFRsgumKhC1YUEJXepIWEEnrvBALJzPz+2N/3nPvM5oZJMilz83799Tmck5tLJjNzT+ac+zg8Ho8BAAAAAADsKai0TwAAAAAAABQfJv4AAAAAANgYE38AAAAAAGyMiT8AAAAAADbGxB8AAAAAABtj4g8AAAAAgI0FF2RwBUeoJ8yILK5zgYVsI8u47Lnk8MexeAxLzznj1HGPx1PdH8ficSw9PI72wOMY+HhvtAeei/bA4xj4eE21h/yeiwWa+IcZkUZbR1f/nBV8ttLzk9+OxWNYehZ5Zu7x17F4HEsPj6M98DgGPt4b7YHnoj3wOAY+XlPtIb/nIkv9AQAAAACwMSb+AAAAAADYGBN/AAAAAABsjIk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNMfEHAAAAAMDGmPgDAAAAAGBjTPwBAAAAALAxJv4AAAAAANgYE38AAAAAAGyMiT8AAAAAADYWXNonABRW7g2tVD409JLo29j+Y5WvWj5A5VoTK4hxzsXriunsAAAApPSp+tpl9y0fqfzWyUQxbtE9rVV2bU0v/hMDYHt84g8AAAAAgI0x8QcAAAAAwMZst9TfEaz/S87qMT59zfbRCaLtinCrXK/BUZUjhjrEuMNv6WXj61p/IfqOu7JUbvvVKJWTnlzh0znhf7m7tBTtcVMmqJwUIn+V3aa8vv1Ulbe3dolxTyW0898JotRk9W6r8muvvyv6XrznfpU9azaX2DkhbzvHtFd5230TRF+Iw6ly56GDRF/43FXFe2KATTirRavsqBwl+vb2qqVydoxH5aR/bRTj3BcuFNPZlT/OJg1Fe971E1XO8YSo/FjV7WLczOY3q1xpazGdHHzmaNVEtN0V9HXngesiVd4yfJIYl+OR152F0XVzb5UjexyS55GdXeTjl1eO0FCVL9x2lcrN/yZfDzPayO3EgYxP/AEAAAAAsDEm/gAAAAAA2FiZXervbJQs2p5QvRzqYJcqKl9slyXGRVfW7WVXyeX3hfHthUoqvzbhVtG3stlnKu/OuSj6Xj1yk8q1lnkMFE7Ozfqutk9P+kT0pYTorRZusbjfMHbl5Kh8xq2X8rQMFcOMS7e1UTl88SbRZ8flUxd7XKNzNb2sOnrK8tI4Hb852lr/DfPFzDtK8UyQl8MjO6i8pM/rKud4KuQ1/L942QQsBTVNVTnjr+Gi78Fmv6s8qtr3Ph2vUdxg0U5+YG0Rzg7CgcOiOSL9XpV/bDKrpM8GV+Bpr5d8Zzyg36PevmGGGBfiyFX5xvBzKud45Geq3tenhfFj0y9VbvHJg6Kv/pCDKruOnyjy9ypPzFvCF0+crPKybDk9HlNfX1fm7t5T/CdWjPjEHwAAAAAAG2PiDwAAAACAjTHxBwAAAADAxsrUHn/XdVer/Na0iaLPvJ+7uJlLb/xj/AMqB2fJTaftvxqmcqUDuaIv9Lje8x+xZqWfz9BenFGy3FBWZ713ceTb+j4K14ef9/pK679bTTul9xT/NEmXD/vtn+PEuB8/1Ht6Gn86TPQlPhPY+97zcrCz/plFNDitO6aUwskURZBTND3x+vnWNTZN9P3k6GCgdJ2vq/c4RgeV3Gs5tMu36Pul7OmnH48hVy8V456omm55jGYfDlc54pB+PzzdQZY6qvcf/TpT4fs1BT9ZGIZhGI42zUR7x0j9urekky6FWd0pb14TZHpvXHihqujbdSlWZXP5uE86fyDGvdhmgMqe1fL+NygY1+kzor1nv+keVk0MlDGel06qnJY6uxTPJG8bOsgLtlvaDlU5dCF7/P3h2jA5p3s5XpdIDWKPPwAAAAAAKKuY+AMAAAAAYGNlaql/6HZdkmJtdl3RlxJypEjHHnWonWjvOq9LOExrMFP0nXHrJYxx4343CoNKVL7bP722aK9uM9FipO9eiF2t8ncV9VLvgZk3i3EfJyxSOaqx/ZdI/avbVyq/tu3mfEaWbc4G9UQ7rYte+tZiVX/RV4tlqiXu/N1tRXtWz7GmlkOlyadTxbhF9+jl6JF7toi+ohdEKl+ODW4v2uOf1q+rrUP1drYgr7//D8i8UeWWlfeKvo0PjzXy4n2MDtF9VY72rZpcueasXl3l9LH6/fDrDpPEuMSQEFPLqzatydSz+vppbq9Oos9tKo382AK91N/8O2EYhnExTpcIDLP8TvCFMy5WtK9tZL2dBqXvwBLT/CPVetzybP0cfPCbR3SHw2tgPhOCdlfr34WpCT/4eIYobk6HfT8Xt+//DAAAAAAAMPEHAAAAAMDOmPgDAAAAAGBjZWqPf+6hwyqPf+1u0ffyrVkqO/+oqPLGoeMtj/fS8eYq77gxQvS5Th9S+b72Q0Vf5gid6xsbr3DWKIzcG1qpPKPFBNEXZORd7mvgnq6ivWZRI5U3PSSPsfii3pUYu0aXettxSm7YCnllsf6+3vuybCjEkXvlQQEg+MMLln0Xd0ZZ9qH4ZHe7RuXn/y3LDaWE5P3k+viDW0W7xtbC3VOlPHOYSt1m33iVyrP+OkaMqxWs96M+tOcmlfe80VCMi1y4QeXFEfGib+mcFH385PmW53R2QzWVoy1H4f8c6K/Lu23pYr6PQsj/Ds7Dp2flPZHm3qnva+PaLveTO1pSP67EVYoUzdujV1sMlI620q+bVf5IEX2urdwnoLjEv6pLkPb8sq/lOMflHJWTdxeubPfpGP1auWhFJZVvDD9n+TU3bOoj2lGL9f1wuBeOf7g88ieZE6Gny9Z3VwkMfOIPAAAAAICNMfEHAAAAAMDGytRSf7PoqctFu/rXejmM68RJlZs0fVCM29JZLzGd/34XlWNPWy8hdSyXy/nrL7cYiCJxd2mp8rgpeml+Uoj8NXSbFit1T+upsrN3lhhX5U+6RkrjT4aJvpSJ+1QO2rde5arL5DnlvKxLGM1qLpcnP3i93vPhXLzOCETuTi1E+9qwX0vpTPwrIdK69GLdRS7LPhSfQ/2zVb4+PNur16mSuVxcjbEs7S+qQ8N0CcRVo83LxOWCxLt33KFybi+9RDXiuFyiaq48dXBQK9G3Mjnvcn7fXqgk2knv6ddfe2wuKl61u2f6NG7m+Roqv5Wut77FPS3rhbm2Z1ge41QztkKVNNeO3aL93Nd6qXavvtbli7fcN07llmceF311WepfbDw5l1V2bd9RrN/ryF16C0ezCvNMPdYLyg8elBuoKl7Y5e/TgpejrfS2q7rfluKJ+AGf+AMAAAAAYGNM/AEAAAAAsLEyu9Tfm+t43kt7c87mfQd4wzCMJv22qnzsXafsdLMcuLg5Wsm7Bx9/Ut9dP8V0J+q1l+TX/Xy+sconPtd3K652Su7BqPzpCp29vndhlpfGOeXSqhNP6DvHxy72Hh0Y9nQLF+1YZ4TFyLIvOEHfYbx3tPUdxcN3nxJtnunFI7hObdHecu1UlXM88qe+Ta8sN/a+pZc2RhqFuxNyeZYxvq1ob79LV7Yx34e40Y+DxbjU0ZkqW72fehs8ZN6VBxmG8dLLA0S76j72yxXII/q9p/Fjw1Wu+6N8HkVu0ZWPYvbopd4FeY27EFcOyteUcQ1G62sXw/qm8bChY0Pai3Zq/zSVva9BrTR6Wm4d4Rqn8Dw5+uIkPUdvUUwJCRPjLta/bNgFn/gDAAAAAGBjTPwBAAAAALAxJv4AAAAAANhYwOzxt9LoGVnSZGAzXeJmar2fVO5y92NiXKUvVhjwv6AIvYc89/Wzom9F6myVd+fq/TJPPjtKjKu6bK/KsZFHVS7pfUzX1NyjcmYJf29/CU46Z9mXnValBM+k6Pa9E6lyx1C36PvobB3dOC1/7+A/ziYNVW792Wafv67PbF0as8EsXnsLaueb7VTefpcs/3XGrfcl3p12n8oNh8v3Rte5vF8LgiIjRftE7+Yq96g4Ro419D1DUr/S76lJ09jTXxTmcm9JI3dbjvNHacScNtbvCSh5IQ59/6kcTz4DETCODusg2gOGfKNy/6g3RF+lIOv7lJm9eOxqlT2X7LPfvLS5jug5xoiduszmd6m+3d8mEPGJPwAAAAAANsbEHwAAAAAAGwv4pf6u02dE+8SQRirvna/Lx/3lpeli3F/v6amyZ70sBlf3ZdOyRQ9rrwriYhddwu/71EmW4x5+fKTKlebKpb/+WM6IK4td477yoBLgjKmm8pFeKaIv+p79Ki9N+cjUI0utvDvxTpVjj/zu3xOEsqe7fqxmVlvv1auXrN638w7Rk/LqTpUpPXRlzrhY0f64p34tdRvyeWte3l/hpj2mcdaCWuiSqU2nbBN9L8WNM7VkeamOG+5VueE/9dfxmJa8vf/Qy4lzI7yuU8wV+7y67krOe1vGsP3XiXb4d+usDgE/Mpc+9X5uo3SYt7SlD6yqcpdOvm1vW1B3vGjLx9V6af+OHH312+dduQU2fs4RfbxzOw2gsPjEHwAAAAAAG2PiDwAAAACAjQX8Un9v7o16+eG9/3pK5f88L++kuaGdael/O9FlNIkcpnLyB4dUzt2V6Z+TtLHmL25QOcjr70oD9+iKC+FzV5XYOeUnvzvqOh32XuB4MVo/PpH5jPPmvralyh6nXlO670a5LPhyrRyVgyro5Yw/XCuXwYWYlqUedslj/H2X3pJz0q2Xy0UEycXFcSv1nart/aiVvJMD26s8Z7D5Lu8hYtzgfV1UzhkgH0fXsb0GfOcIkz+/1qHWi+nDR+ilo456dVXOGFxHjLv5Rr10e2Ts+yrHB4eLceZFqS6vrW6OL2J03+kMy3NC4TmjolTOviZZ9IX8VS/3/SNVvo6KceJ9zfp3Z/FFXYVn/6B40efJ3eY9HLAlT8cWov3A1Dkq94g8XogjFu4z1RE79F3la78mtyyynap0VYy+UNqn4Dd84g8AAAAAgI0x8QcAAAAAwMaY+AMAAAAAYGO22+NvFj1Fl60Ztv0x0Rf1qi4TNiPxe9G35f4JKqfWfVjlhv+SfydxZezyy3kGstN/bi/az8Xpeym4vcqWrP1Bl5CKN8pGybX8Sul8t02fb7KxzghEl7LlPmy3aQf81GffVnn+MLnHLT/PVPtQ5SBT3aiLnsti3EGX/tlOOHadyjcuekKMq7Je/57U/OGI6HPs0c/TY9v0XuQ4Z44Y51m9yZdThw/MpYwMwzB+f2mCqSXLKJot35+gct1M38oeIW+e7EuivfKSfh63DZW/+/MWfa6yr+XAFl3Ue/UzvG5ucn34eZXXXJav4VWm510KDgXjCPW6H0qXZiqPnPSJyteH/yTGHXHp34vFF3WZsX+k9xDjZjSZpnKtYPm9zMKC9O/SrnuqiL7E7fq57s7OtjwGYDdO03WS972qfGG+x4Zh/O/9o6x810jfW+DafnLOUvk/K7yHowTNuvoDlYcbHUvxTIqOT/wBAAAAALAxJv4AAAAAANiYrZf6mzl+2yDaF3rHqtymz3DRt/KZsSqnXa+XNfdLuFmMO9PJn2cYmHJlJSijcpBeGro8Wy4xTJx+UH9dsZ6VFBShSxalvdHUq3etSv123SZ6Uh/frXKgllJJ6r9etJv8W5eqrNvmQKGOufhoisrHvtUlw6ptkUuQK3y32tTSfSnGGstje/+cDzzTQeU2oXqZ8efna/t4tiio9GcjRDu/cmBm8a/qTEnFonEdOSrazw/RW87emDxJ9DU3rcb/9Kwu5/fS0u5iXMo0vVw7+MgZlWNnnBTjrq/7s8oDFj8s+vJ77iJ/QWF66fyJPi1F37JXxuX5NU1myGuTOov1czF0oX59rVbzvBg34/tWKo+qZr3txrxt5I8H5Dm03zdC5bjpG0Wf+4J9SluVtvxKCptFdThq3Yki8Z4ffHTnrSr/5YFqKsd/L7czOi8W/Eo24yG5/TLt1ncLfAwUj32/6vdPI7X0zqO48Yk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNlZs9/t7Meyjjxsm9U9lP6307EQ69gfKDhAViXLeeuixZxJyV/j7FgHfCVVG0c3dlltj3Nu/r3/6qLpWU1mOCGPfthcoqH5yYJPoqnbJf+ZT6f/VvOa6axl6/Hs9bROdjef77c4t7iXaKsapYz8Pu3F30nuOXWs/16Wtu2nyvaFdcQwm/4lLhe723/tn61/j0Nfk9J8710MdYGD9P9OV49OcB4ZmynB98512yL+2t5jr3yHtPv2EYRo/td6qcMkaWDDZftwTX1fdXuWq+fB1+qtpWlc+45b7ktrNGqVwzVR/vp2ZfiHHL/67PsU/fbqLv+Dj9nhp2Qt7bxcy5JDDL4Jak/EoKmy29aoZod2/3kG6s+MPv51Weubamq5z4tH+P3SijuvyHW/Meh5JXcZ/1TTYqOXSfs3GK6DP/vgQCPvEHAAAAAMDGmPgDAAAAAGBj5Wapv7tTC9HeebcurdO0RaboMy/vNxt/UpbgiZhHaaP8jP7tbtFOMZXO8zfzUmXDMIyjT15UeVtrvby/66Y+YlzkrXopZSXDfkv77arePArG+dPL095XuWmI9c929KHOKlfue0r0BWrJy/IoN1z/zd+7XKN5uXH9aXIJeUmWYQ1EjmB9SbX9natEX1r3iSrvz70k+rq/p9cTJ0zZqXKuV1nHnBt1mb6mr+lSrc/HyvfWqWfrqfzJ3+4QfUmz9fucM0aXKrvuJlk6MKuPLvk4p+UHoq/OOLmN4f8syKom2u+nJOY5Dlrqz7pk5tYb3s9npJQ+SF+npnDpEjCO3JV05UEoFUH5vME5HQ6V3eEh1gMDAJ/4AwAAAABgY0z8AQAAAACwMdst9Xe0bqpy+gjTHfk7fizGdQ6Td7q1csmj71i74mR92ek+VIgztBmHbAaZ/pY0tpO8C+1EQ94Js6j2vNBe5Vn3vyX6UkL0Y3/1qgEq1+q51QAgtaxgvfTbbPnUq1WOPfV7sZ4Tik+lz01rg98svfOwm31P6WoJad3Hir6DpuX9d7/6lOhLmKu3nJ28QV9nePpXEuNmNtXHrO7Uy+2bfC6X6ae8f1zliO3WFYdcx0+oHDXjhOiLMr199x4qb20e13tP3gccVcXrH7ZYfm/8V2h6uG7cUHrnYXfeVTZO3623h1adJ39P3efO+fV7HxrVQeV5I1736s172wxKXtVpuurV5Kfrib7BlfVrXsZIuR08qX/xnpe/8Yk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNBeQe/+D6eu/FzoG1RN8/+3yucq+Kx43CePZIa5WXjm2nctWPl+c1vHzzqvxlLgXVJVzuGXximi5F1GCqHhdyWO6nOtKlusrRffarPDz+JzHutghdwmh+Vpzou3/TrSrHvBdpefoIHE6H/jvlqRRZTqXGtyV9NoFv30x9P5QQxwafvqbmEv2aSvm+wHXu3namVvGVWS1v3n1kkmVfmOl+OHcM/kX01R6hS2MOiPo6n+9g2tf/2QiVk/66Woxy5fq38GLsJHk/D4/lf/OAX79veVD3Rf2zndGvtujrV8n6PlK7b/1Q5duu6quye+M2P55dYMu+Q99zo/JoWZp0adJ4lXuu7iv6jO0F3+MfXLOGygd6yzKWXwx/Q+VawdZ7+o+49H1AQi5Ssrg0vbHiFtG+tes7Kqc8mi763EZg4RN/AAAAAABsjIk/AAAAAAA2VmaX+gcnxIv2mVY1Ve7zwncqD64yu1DHH3VIL3VcPqm16Iuetkrlqm6W9xdWmEP+em27abLKv14bpnLGpRpi3MDKmT4d//GD16r83e8tRF/y4yu8hyPAuTymBVX8ybLA3F1aivY7LT5V2VzC74w7W4xr8+0TKqfuoRymHZxJ5AlUHH45n6py29BNoi/aVH7v2RjrrTXd0u5See/yOqIvceYZlZO26C0aHj8v7UfpmLa3g2j3bfKV5dgcVoJf0S0vL1V5VLXNluPSno2S/3C+bYG/170d9FxhbuxC0ec2QryHKwMy9ZLyHVMbqlxtNnOPssRlql3uvpidz8iyj3d/AAAAAABsjIk/AAAAAAA2xsQfAAAAAAAbK9U9/ubyF4ZhGCen6LJrQ+ovFX19Kx0p8PGHHeik8rp35R7wmJl6v0/0OfbSFFbckqOi/cyj7VV+rYb1z7Vz2GWVO4VlWo5bf0n/barv0kGiL2Wg3uOYbLCnvzy50OZCaZ9CwMmOriDancKyTC2nSt9fkPdXSRmkS4UFWtka5K32Uv38CRnmFH3sHS6836/X5YXb9rtB9J25Sr/nBR+Te35TJusyeMGH9XtqQvY+MY7nn71dmiaviY0xpXMe5c22G9/z8xHlZ6rLs/X9PR5Zeb/oS3okQ+VqWcxFyqoGweEqnxh4jeir9lFgPW584g8AAAAAgI0x8QcAAAAAwMZKZKn/5Vt0ubzLI0+q/GzSN2LczeFZRkEdcV0U7c7zR6mc+lyaytGn5VIMlsz5hyt9p2hn3J2gcuPhw0Xf1nvG+3TM1G+Gqtxwkl6SmrJ+bV7DUU44HfydEvAHx2+6nNy0s7Gir28lvez8QpOaoq/Cvv3Fe2IBznVCX9/Ejftd9MXl83UU44NhGEbVDSdFe+IpXd7tsarbS/p0At7PIzqqPH2oXJ69seOUIh//07N1VT6UU0XlKes6inFJH+hyuYm/yVKezEXKpqld5O/HKbeea8b8cV70BdruOK6kAQAAAACwMSb+AAAAAADYWIks9c+8U/99Ib3ZVz59zcTTDUR77NKbVXa4HCqnvrRbjEs+slJll4GSlrsrU+WkkZmir/vINj4dI8XQdxEPtCU08K9Li6qr7GrBoriiiNpwWLSH79d3HZ9cd6n3cJQTb7/XW7T7jh6rcs2/7xB9J043140VfxTreQHljWtrumh/3zRKZyO/66dtxXRGgc25ZJ3K9VdFiL5WIx5X+eNH3xF9TSvoOcYNm/qofGaJrLpQ7wu9LSp39x6Vkw22pQa6p7bJ98Xe9darHJR1SfQF2lyTT/wBAAAAALAxJv4AAAAAANgYE38AAAAAAGysRPb4pwxZpXK3Ia0KdwxjVZ7/Hmh7KwD4rsbbuiTW7W9frXKisSGv4ciHeQ+iYRjG/nY6dzMK97qMwFf7E1kmrM+d3VT+ImmB6Ovyj74qR99XWWXX6TPFdHYAUHTuCxdEu/ar+tri2Vev8R6uVDR25ZkNgzKcdhbdTd5v42cj0tSSfYGGT/wBAAAAALAxJv4AAAAAANhYiSz1BwAAZY/r+AnRvtyrmsqN3nxU9G278T2Vu6c+pDso7QcAQJnHJ/4AAAAAANgYE38AAAAAAGyMiT8AAAAAADbGHn8AAGAYhtzznzxA7v/vbrQxtdjXDwBAIOETfwAAAAAAbIyJPwAAAAAANubweDy+D3Y4jhmGsaf4TgcW6nk8nur+OBCPYanicbQHHkd74HEMfDyG9sDjaA88joGPx9AeLB/HAk38AQAAAABAYGGpPwAAAAAANsbEHwAAAAAAG2PiDwAAAACAjTHxBwAAAADAxpj4AwAAAABgY0z8AQAAAACwMSb+AAAAAADYGBN/AAAAAABsjIk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNMfEHAAAAAMDGmPgDAAAAAGBjTPwBAAAAALAxJv4AAAAAANgYE38AAAAAAGyMiT8AAAAAADbGxB8AAAAAABtj4g8AAAAAgI0x8QcAAAAAwMaY+AMAAAAAYGNM/AEAAAAAsLHgggyu4Aj1hBmRxXUusJBtZBmXPZcc/jgWj2HpOWecOu7xeKr741g8jqWHx9EeeBwDH++N9sBz0R54HAMfr6n2kN9zsUAT/zAj0mjr6Oqfs4LPVnp+8tuxeAxLzyLPzD3+OhaPY+nhcbQHHsfAx3ujPfBctAcex8DHa6o95PdcZKk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNMfEHAAAAAMDGCnRzPwAAUD44WjUR7QdnLFA5zJGj8sTklBI7JwAAUDh84g8AAAAAgI0x8QcAAAAAwMZY6g8AAAzDMIyMj69W+fPO74m+qyrofOvW3ipXMPxWvhsAABQTPvEHAAAAAMDGmPgDAAAAAGBjTPwBAAAAALAx9vgXk2q/VVU5yOFR+ViH06VxOmVbu+aiubt7pMrP9/pS5bfSu4px5zZVszxkgxfWq+zOzi7qGQKAbQQnxKtc/6sjom9BrQ9Udnt93Zsnmqoc8YAu55fr39MDAADFgE/8AQAAAACwMSb+AAAAAADYGEv9/ST9o9aivTp+rMrtlz2mcqKxocTOqSw78JcOKn8z9HXRFx9cMc+v6dfqS/kPrayP32ntoypHzlpZ8BMEyhBn1aqive+hRioHm3aynG5xWYwLqajbv3Z8V/Q9uFOXY0s/XL3A55R7NFy068/TC76Df1pb4OOheDlaNVH58utnVX6z1q9eI/XnAc2njRA9sWv14v+IA7yuFjuHQzRPfp2s8pfNpqj82E0DxDhX+s7iPS+gHDoyooNoe244pfKAJP16OKjKVstjjDwgt6we6BahsuvYsaKeIsoQZ5XKot1isf596Rq1RfS92V1fj7m2bC/W8+ITfwAAAAAAbIyJPwAAAAAANsZS/yJIf/calVff/LboO+fWd/KPWiqXxMIw6n28S+WDg+TPJ94Pv5UfvKkfj4eCn1S50hcrin5woIRt+3eyaO+4Y0IhjiKfZ/OSF+pGslFkub1cKo87lSr63l94s8pJn+jlbu7NaUX/xvBJdqxeUvp96jSfvibigFxqHjGb5f0lyVmpkmi/nDpH5fhg/Xju6xEnxtUaw1J/oLCC69RW+cJUfUG6uvF4MW5bjq5s8vSuXir/cLSRGPd+0hcqT66zTPRNXlpP5fmNrStVoXQ5k+qrnFOziuW4kOPnVT5wi9xC+XWsvm774Exd+YWHS26bB5/4AwAAAABgY0z8AQAAAACwMSb+AAAAAADYGHv8i+C6lttUrh1DpPkAABoQSURBVBRUQfQN3XOryjHvLS+xcwoUuYcOq/zQB8NF36IhurxfTVNpv/lZEWJc98gLlsdvVEGPPXSTLjNW6Yu8RiPQORunqOyODBV9Gf0iVZ7RQ+7RM3tg7UCV6/be7MezK7qXrp9VqK/bcFn/7r958JZCHWPl7gSV29bPVDm54lEx7h8xm1R+smqG6Huyv2533DRU5cpl68dsK+byfYZhGEPH6nKoQfn8zb/j34apHDvtd/+fGHzmOntWtKcf7ahy13o/q5wd4zFgP5kvtlfZHSL7whqeUXndNZ9YHmPy6USVFzSpajkOWssFe1XuXXmNyinz5LVq4xf1OM+hA5bHG9RmiMrvzpRldR+urO939fabf1K5wSjuR1UcPB1biHbmMP3a2bT2Qcuv61fjJ5W7R56yHNdwjr6+iU+Rx3M69PvuklMNRZ8jLMzymP7GJ/4AAAAAANgYE38AAAAAAGzMdkv9L/bQJfZiRu1W+VIfpxhnXmruq6NDO4j2a3G6ZNynZ+uJvlN/jVc5yDhR4O9VntT5t1xOOrVvK5Wfjdmu8o5LNeQXRu4yfJE6TpfXcBfi/FA2nL+7rcqHe1wWfQs6TVQ5JUQumXIb5mWw1n/rHNF4scpzjOqW40rDp/fcLNrjm1ZWuermM97DlaBzF1XO3ZVZqO+dZOhlbeZXstPVZAmxr1fsUfmOCLlE2ezE7dkqV/60UKcEH6QPqCjaPSKPq9wtrafKzsFym1rVDLamlVVpU0xlwv6ll/qHpVi/BqDsuXinvk493kRfhkd1lNun1jcfq7LTIUtrmuV3XTOwsr6GCtoq67ZSPu6/zt/TTrSfr66vJ9qt+7PKKUNXiXG5hm88q/U2uK5zR4u+jN6TVH7tjs9Ufn9UogH/29dVbhne0tl6+6fZKbe+bmm5cpDoe6u53ka3veckw4rLo5/DaZ/LksdxB0puWx2f+AMAAAAAYGNM/AEAAAAAsDEm/gAAAAAA2Jjt9vj3f3WBygOj9ql8Y6shYlzYgoLv8R/w2Dei3SJUlw175MWeoi96GfskC2v2+BtUdg/Xe2Kei0kr1PHcYSFXHoQyIfOL5qLdPVnvjXs17l3v4SZ6X39mrizzePMyXYIncn24yrUnbxTj3FlZBTnVEuXeuE20K5tOPb/9ncV5T4tD98o9andELLIce8qt7zVQd4rTchyKpuEa/Vr3Sdxbom/meX3fGcdofY8IV8aW4j8x+EXssmN5/vsvrT8U7f6J96lc2Ht74MqCExNEO/YzfReU3jGrLb8uNeRXlesE6+tI7zKbzx3V9wJ4Idb6ePkJcejX27oh3vebYo+/YRiGy+sScfrZ2io7Z/n3Z9Tgq2z5D711rB6s743jjJHf13Wce4UV1o639T0cfu31ulevviZs/vsDKmefCBejGr+sS/PV3iffM8d06a9y1FT9WtxKVpY2Vl/S95uqOcXr+jPvUy8WfOIPAAAAAICNMfEHAAAAAMDGbLfU/9DlKiq7DV1eKjfcuhRKftxdWqrco6Is+5Dj0UtBcsMKd3z8r2of6G0Syxc1VHnM1zli3FPRO3063vkX9BLuircW8eRQZMG1a4l2xhu6dN62TlNF36bL+jH/+9E2Kv8wsaMYF7PhnMpBWZdEX9K29XmeB6UdfRMUprdRZEzRy/t/v3aM18hww8q9f9bbLUKWrPXbucEwTj3QXuU3a05Q2W3IMn3P/dRL5UZZetmoqxjPDcXH6dCf20QFyRKme+7Rr7G1X80sqVMqF8xlZZ98eYbo6xbp63Ls0Dz/9Y4eA0Tbeeikyj1qDhR9WfGRKo967T8q3xZxyrDy4cHOXv9S8C2vdlR17ibRnvV1iu47699tu85s6yKAHUP1VcmeRxqKPu+y1/CdO0K/y8U6ZTm/uVl6zpj4jL6OzN0lfyfMj1pQi8ai74zp69qE6rngIZfcdvrQh0+rXCer9B5PPvEHAAAAAMDGmPgDAAAAAGBjAb/UP2NcW9GeU00vx3/3tF6uU2XFATHOerGNYTir6DseHx+tl4nXCpbLs0Ye7KBy3Edy+arHQGEdHaZ/rqeb6kdqftU5XiN9+7vVyRU1VK5o7CrSuaHotr4ol/qnd35P5aQfBom+Rk/qx8t1Si9hrGbI5Xfm5xtLl4smq5d8TT1xr16utr3DFFOPXNp/3qO3WHScMEr01V2t72DLFouiccbFivaxDvm9m2khp/XdvV3pvm2T8rb3ef3anF07x3JcyqDC3YEcvnN5rJ9JbgrZFJuaI/Rzx9el/SddcvtZ1/f1kt8aK3RfyBp5HSme2QcOir6DT+htqPkt7595Xl//uPpRUSUvJVrRZ1OGaI4/najy8Cr6eudCovXrKwomYba+QhzfKVH0PVZFP5+ff0Nvn4l/sLIYZ8REq5jz5jnRtSx1rsqbLutn7b3Tnxbj6r1SNrZr8Ik/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNBeQef2fDJJU/6fau6Lvg0ftiZv/tZpXD963y+fgZk+qrvPnqD1RedLGSHNdG7tuC7xxtmql858c/i777o95ROSLIXJKqcH+nSpitS+Kwv7j4OKOiRHv7C7rkyb9v12WP3ni5vRjX8ZdhKqd+9Yfoc5Xk3rtyLOfm1ir/MFaWLQ11+PY24fbofXQV98lnmifXt33o8IHXz/LaZttVDnHoPbw5Xjeaqf2Lb4/BnhdMz0+PLFP7Ql9dNqxn5EnDSshBfR63d7lL9LkyuM8KAse5Pu1Ee3L8G6ZW3mX5DMMw5mXFqDxp5D2ir+7Cou/1Tax53Kdxz/2in38p+7n3RmnzXJLzhvOuMIuR8JewZVtVnrRJlrR87Fq9x/+t5l+q/LeeD4txf/mrfu/rHml9T437poxUud6LZWNPvzc+8QcAAAAAwMaY+AMAAAAAYGMBs9Tf07GFyvd+tEDl1qGyeFfqd4+rnDLXt+X9mS/JpcdrOr9laukf0TMfPijG1TbK5jKOQHCiWUWV+1SS5U0igiL8+r22j9LHSx7g10PDJO3fjUR7+50TVW63rq/KsTPlcn5zKR22YpSO3b31km5fl/Z7iwrSSxZ/e32S6Ht29NUqz/pJL51NnJMtxjl+21Co712enLi9oWjPiR+nco5H/y1/flZVMS70iC7LaN4F4O7SUoyLbXtY5R+bfmlY2Z+rl6x+kyWf+4MqZ6qc8vle0Zf+Z11m17U13fL4QFlQ4zFZ+tK7rLPZsP3XqbxtTFOVIxeuLNT3Dq4Rp/LBXg1E3xdJY0wtvSXSfA6GYRi1v+fzvbIkKEJe38YEH8t73HlKL/qL+4J+78s5Z/38vT5cX4/8/tIE0Rdk6Gsk7+vUJr/ouWHSl0dVLqulpXlFAAAAAADAxpj4AwAAAABgY2Vqqb8jRC9XOjSstehbM1rfaVreuVj+7eKuFutUnv+aXsKf9K+NYlxQjViVu9++QvQ5TUs6Wvyul3DEv8rSfn+JnrJc5Q51Rou+ZY/oJWwxzsgif6+acaeLfAxc2a6e74m2y3RHcOfMaiq7s1jeW9bUm6vzHcndRN8/E+ap3KpC4ZYfvhKrX5df6atzbl+vrVoLh6rc+OXDoi93z75CfW87cFaLVvlcgsNy3OKLervFU9/eJ/qS1+v3OUerJioff/KiGLeq6UyV116S76+P/tFf5ervhKt8uYq8lBg0UVfbSQ4/IvrSjUTL84fvnA792Lg8bJIqLnunJ4n2K8P1ttOdWdVF36l+lVWO3F245f1m6U/o58rmP4/16tXXy++c1BV0Dt4bI0b54zzgP57G8vXvkcq/5jku/nvfF4oH16mt8pl2dVQ+3Fa+fid9cU6fx5rNPh/fTsL2hRT5GN3Seoh24hu6Yo5r+44iH7+48Yk/AAAAAAA2xsQfAAAAAAAbY+IPAAAAAICNlak9/ocH6339q0bL/UzmHWw5plpE08/WFuNeqaH3M73SX+dnb2wrxt1U+VuVrw8/L/pWXtL7JOPv3nTlE0eRxL8g751wx45RKmdXsf7blMf02ztr1Ouir0FIRQMl66nDsizYK3FrVH7+71P1v198QIyr+KW8xwZKXug3q1V2fSP7/tlI7xW/XKOSylk1K4hxJ7rrkjlbrp0q+sylcMyCDXnPgB1/0veJGNjsOtF3pKNprLusFsopHqdu0SXw1g/23uurDZ33kMrJo+TzKjghXuXLr59VeUXqbDFud+5lle/7dbjoazg4TWVXi2Q97pXvvY6hyyK9ueYm0Ze8dZ2BomNff8mo9uFy0V7xoXmPsPf9g4p2P6HT98vS0sv7vWFqydfbC+4clafP0M+xOru5F1Vp8y7ZZyTXU/FAlyifjnHT67+I9vQHr1G5f+pq0dc8fLHKf4rQ85nM3AtiXPfER1Wu08un07AFR7CeLFRpL+85Y3Vt4u32tO660XW/6PMY+41Awif+AAAAAADYGBN/AAAAAABsrFSX+h8bLJc1/f7MOyqfMy1jMgzD2Jqjy7r9bbRerhJ24rIY99MrmSpPTfhBZfMWAMMwjCDT3zy8F8y1rqCPOXLHNpXH9rpLjHNv3GbA/6I+00tU810U5dBLdG5OlCUBd94zWeWh9Zeq/J/GXcU411ZKy13J5Vtkac2wpboMjDtbL+nd+qc4MS716cdUTrtnov73MW+IcUMzh+jGKrbWlDWubRkqO00ved7PzajPdL5mmFwifsNA/Zx+vcYawxdT45eIdqOX9O9T/WeXG+XJiWa+LUdsMMp620z9r/QSxzdr5V1CyjAM4+HHR6qcPHeV6Lt4WxuVv/9wkuUxUhc+oXLKoNWW41A8YjblXnkQypxf/z1BtN1ey/vNrp2gr3nqvMby/qIIqlRJtB11a6p8tL0uRXyijdxi1rdt3q+3sRUOivbwKtavt1aeiJbXQg1bHLIcO/Lr+1Ueu1DPnSocyRLj6mzeUuDzsIMzX+utFr80/1L0+bphym3aEhDon5gH+vkDAAAAAIB8MPEHAAAAAMDGmPgDAAAAAGBjpbrHv/H9co/8/Cy9R/iV9/uKvppv6j1MEYbcr292YlRzlUeOv1blt2st8/m8nKa9409t0jUvam3c6vMxUPyCwsNVNu/p93bOpcszGrnlqwyYr4ITE0S79Ry9r7t7lNzL+9Bbev9u3Hj9vMw9dFiMS33TVH7tHh3jg8PFuEsx+vEJ9fmMUZbFTpB7Tre8p/eqPrysi8of1l1q+Kz+hSuPsamcyvp1K8jr7/VdN/dWOdzYrbK7iyyv2TN6ep7HaP6BvB9D/Fz92DlaNRF9Q8fq/ZH5HSPln+w5Lk0Vt55QmXe8si1joi41HeLYIPrMpau91flJl+TMZxj+P+99/GljGqk8uvO3om9w5QK8L/1/O3MvqpyZU0X0XfTo+4aFO6zv29Dol4Eqx78vS906F1uXQU0y8r7XQHkq+OlMqi/aGYNqqJzWXN9jyvtn8vxR/T45K72Fyps7yZLEzasc0H1FOdEygE/8AQAAAACwMSb+AAAAAADYWKku9V/7fWPRPvl5jMo1txduqeDFOL1seHj1n009IWJcuxeGqRyzUZa8MKu7Qy/vYMlc2ZL2tnkZqvXvy9uzu6uckF6+yoD56plFc0U7Ofi8yl3ff1r01R3v23Nz2zN18vz3PjtvFe2IVbtU5jlmT54cvdRxyaardEcBlvo7dkb485QClttrsaLb41upvxyPfrt3G7oMp9HknBg3YkeaytWdshTfV6euUXnan3Rp1PrH5bY9nseAtaAwfZ3atOkelXM88pljfq63nPS46Ku7znrLK/5X+MIw0d7RQG8PPeW+KPpuT+ujcsY+vQW51gI5ZXJm600Wkemm7TXpO8W4zG1HVX4oar/o+/x8dZWThu7Vxzh1Ko//BawcvL2maG/tN97U0u+RjT8ZJsYlj9mucni/irqjkzz+N7v0fCPeCOyy03ziDwAAAACAjTHxBwAAAADAxkp1qX/8v+SS4cIsD3RWry7a+3vlqpwUou8R/p9zchlIzHu+LflmyWL+gmvXEu3L0/WdSI/Priv6YicW7U7P3neeX3Tr26ZWRcNK4pd6yVR5ustpQTz05RDR/uW+MSpvGjJBDpZDlWln5e/CA1Hvqjw3q6rKZ5+XvxfO49Z3q4X/eD9/tj+m73pbOV0vhfP1tbEgHMH6raZt4535jNTMd0I2DMOosbL8vhrX+9p03+4esu+nZl+ofMttQ1U+1kJub0sMOWlq6TtLb+gwRYwz361/7SX52cCyN/UdyCtn5H0naZQ+T7j1ncNR8pxRUaK9d0hTldcmjTX1yOdbm9X3q5zw4Q7R53KX39fDwpid9KNof3leX5O8P+hh0edcoq9Jkg25NN+KO0Q/59InXyP6bo98S+UVl+SWtamD9Qu68xTXQgVxoad+P5rx5BtevXr+1+Evj6mcNPsPOSxBb0l9cuiXhpWcvZGFO8kyiE/8AQAAAACwMSb+AAAAAADYGBN/AAAAAABsrFT3+PtDxqgk0d7WdZzKyy/pPY5fdr/W6yt922eK/B2cJPeurW/0ucrvD5N7vj890E3lyExdLs69YasYl3tDK5VPpup9Or0G/yzGNQix3tdff8EjKqfu3Go5Dv+V+Be5r/u63KdUjmgmy8q82+w/eR6jWdg+0f7T9jt142m9ny54g9xj5TFQXILr11O587wtom9+9GyV72hxi8r+2DkanBAv2lv/ou8nsCNhsvfwPE081Uy0w75eVfQTC1DOS/ruJAdzL4m+WsH6NfLHD/XP1rvsn3lff3525+pSf/f9Olz0Jf+Hff2BYE93/Xpbd2Mpnkg55qwWrfKxj2NE39qWY72HG4ZhGM3fk8+3+Jd0yT729BeNyyNfD7ddrK1y8G+bRZ+v1yRBkXrft3u+fs7tSJXvcafc+h46fxklb5IUsZiyjIV1QFeUNVJCZLnGgXuvU7nKJ/r61hMaKsbt6VlN5dTQQyoHGU4xLvSEfT4nt8//BAAAAAAA/A8m/gAAAAAA2FhALvV3Nk5R+cWen4s+l0cv0hk4f7DKSeksUSwOlSdXEu0RtduoPK7WatE3aNL7Ks86r7cIfHSgkxg3OVEvg6ufz3J+89KtyWfqib5GT6frcVlZlsdA3hKesy7p9rzRyrJPOmCRUVKOjtfL2kZHb7ccl9NYl7QJXpct+tznzuX5NUGV5HM//V9NVP6hlyytkxAsSxj9H6dD/u15d47eArTw79eLvnCj/C71D/55rcp9/zZa9CUO0Y/rxwmLfDreVb89qLJjq3wcq2/QJXGT55bfn3lZ5NmjS4uNP52o8vAqu0rjdJAPVwO9lPzXllMsx316Vpe3jX+haCWPYe2js3VE+7kYvby/6WcDRF+tqmdU3r1Fb1mtlCnfrx5+eKHKg6osUXnU4fZi3OZRzVWOWMLSfr8x7clwe23QcHv0Y+UwLe8/NuBqMW7jkPEqb7ms5xSNfpElHuu/Yp/nJp/4AwAAAABgY0z8AQAAAACwMSb+AAAAAADYWEDu8b9n9hKVe1Y8KvquXjFQ5aQn2Ndf3EK/lfv4v75L7/H/aVYb0bdl+CSVe1U8q3PDb7yOar2vXxwv57LK8xtX8+o9YwDlXfYvpjJSLa3HfffZRyq/cFyW0duZVT3Pr2kQeUy0F8RMMrXy3tPvzbyn3zAM48+jRqkcOZe9kHmp/Kl8Xzvxqc7dfLz/Rj1jkz9PCSXEna3vv3H0cpTluNrXmUqrvlicZwQzRxv92rnrSYfluA/P6PszLOxt3g+e/r+D4RezGsWK9qvj7lB5Vc+3RF+I+d4zqdbHvHVTf5U/e+k2laNmyNdop7GuIKcKHzmrXbLsSz+lr1uaLD2s8td1J1h+zaPPP6Fy/enW97kKdHziDwAAAACAjTHxBwAAAADAxgJyqf/L83qp3Lf/ONEX/o318jcUv5RH9NL/oAi53LdhxSF5fk1ks5Oiva71F3mOS8+RZfmeHDhcZZZSAf+rzjf6udWmU1/Rt7rVjDy/5h8xXsvAY/IcViAXPXpbTrMFI1ROmOMW4yK/Z3k/4IuZ21uo/GLsBtEXF65LcMoNOShOR57LUXlT608sx036VC8zr7PVPmXCAknyCP1e029Ex0IdI8rYaWrttByH4hGcbppjdJF9v7XQpd6DDL3tZtPlXDGu17zHVU6Zo0s8yisTe+ETfwAAAAAAbIyJPwAAAAAANhaQS/0Tn9F3W+z+jLxzfDXDvndiDDTuCxdEO+Fvvj02txgtrjzIYHk/cCXuzWkqx90rt960GfCYyuc76+eqY6cc1/mmP/I89tJdSZbft+Iv8hjR2/Tdd1OWrMrnjAH4IulF/ZwaNf0a0bf+68Yq1zFYSl5cPO2vEu3YiifyHNd48SDRTv5BVzXy+P+0gHIhcbLeXtGkwjDRt6j/GJWfO3C7yqt+aCrGJT2vXx/tvLzfjE/8AQAAAACwMSb+AAAAAADYGBN/AAAAAABsLCD3+AMACsb7nhvV311uytZft/e5vP+9vrHRH6cFoBBcW7arvK2V7GNff8nIGBAq2mmpc1Secz5W5eR3csQ4z5rNBoCiyT18ROX6zx4RfY8828nU0vfUqMdrI5/4AwAAAABgZ0z8AQAAAACwMZb6AwAAAAVQe5FD/sMdOr7173tVrrqGMtMAygY+8QcAAAAAwMaY+AMAAAAAYGNM/AEAAAAAsDH2+AMAAAAFEDlzpWh3n9lG5aoG+/oBlD184g8AAAAAgI0x8QcAAAAAwMYcHo/H98EOxzHDMPYU3+nAQj2Px1PdHwfiMSxVPI72wONoDzyOgY/H0B54HO2BxzHw8Rjag+XjWKCJPwAAAAAACCws9QcAAAAAwMaY+AMAAAAAYGNM/AEAAAAAsDEm/gAAAAAA2BgTfwAAAAAAbIyJPwAAAAAANsbEHwAAAAAAG2PiDwAAAACAjTHxBwAAAADAxv4fLEnCVfvw3KwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let us visualize the first training sample using the Matplotlib library\n",
        "from matplotlib import pyplot as plt\n",
        "# START CODE HERE\n",
        "img_per_row =9\n",
        "fig,ax = plt.subplots(nrows=2, ncols=img_per_row,\n",
        "                      figsize=(18,4),\n",
        "                      subplot_kw=dict(xticks=[], yticks=[]))\n",
        "for row in [0, 1]:\n",
        "    for col in range(img_per_row):\n",
        "        ax[row, col].imshow(X_train[row*img_per_row + col].astype('int'))   \n",
        "plt.show()\n",
        "# END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YsRekMVDg-"
      },
      "source": [
        "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
        "Reminder: in order to do multi-class classification, we use the softmax function, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
        "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
        "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
        "Here, you need to turn train and test labels to one-hot encoding using the following function: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lQbkllF8mnaf"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "# START CODE HERE\n",
        "# one hot encode\n",
        "y_train = to_categorical(y_train,10)\n",
        "y_test = to_categorical(y_test,10)\n",
        "\n",
        "# END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jv29YLtVO3q"
      },
      "source": [
        "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
        "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
        "\n",
        "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ptTRSDo5nJyZ",
        "outputId": "bb016a62-e07b-47aa-cdf8-7809fa7cd825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean before: 33.318, Standard Deviation before: 78.567\n",
            "Mean before test: 33.791, Standard Deviation before test: 79.172\n",
            "Mean: -0.000, Standard Deviation: 1.000\n",
            "Mean est: -0.000, Standard Deviation test: 1.000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "\n",
        "# Reshape to proper images with 1 color channel according to backend scheme\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
        "X_train = X_train.reshape(-1, img_rows*img_cols)\n",
        "X_test=X_test.reshape(len(X_test), -1)\n",
        "# START CODE HERE\n",
        "#train_iterator = datagen.flow(X_train, y_train, batch_size=64)\n",
        "# END CODE HERE\n",
        "\n",
        "# Cast pixels from uint8 to float32\n",
        "train_images = X_train.astype('float32')\n",
        "test_images=X_test.astype('float32')\n",
        "# calculate global mean and standard deviation\n",
        "mean, std = train_images.mean(), train_images.std()\n",
        "mean1,std1= test_images.mean(), test_images.std()\n",
        "print('Mean before: %.3f, Standard Deviation before: %.3f' % (mean, std))\n",
        "print('Mean before test: %.3f, Standard Deviation before test: %.3f' % (mean1, std1))\n",
        "# global standardization of pixels\n",
        "train_images = (train_images - mean) / std\n",
        "test_images = (test_images - mean1) / std1\n",
        "# confirm it had the desired effect\n",
        "mean, std = train_images.mean(), train_images.std()\n",
        "mean1, std1 = test_images.mean(), test_images.std()\n",
        "print('Mean: %.3f, Standard Deviation: %.3f' % (mean, std))\n",
        "print('Mean est: %.3f, Standard Deviation test: %.3f' % (mean1, std1))\n",
        "# Now let us normalize the images so that they have zero mean and standard deviation\n",
        "# Hint: are real testing data statistics known at training time ?\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdSNEsUOIuQM",
        "outputId": "3b069fc6-4901-4c00-92d2-494b405e62b5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKeM3rj5ld8o"
      },
      "source": [
        "# Working with Numpy\n",
        "\n",
        "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
        "\n",
        "## Defining the model \n",
        "\n",
        "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
        "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
        "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
        "\n",
        "and our scores are obtained with:\n",
        "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
        "\n",
        "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
        "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
        "\n",
        "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qWKdwetUld8o"
      },
      "outputs": [],
      "source": [
        "# To avoid implementing a complicated gradient back-propagation,\n",
        "# we will try a very simple architecture with one layer \n",
        "def initLayer(n_input,n_output):\n",
        "    \"\"\"\n",
        "    Initialize the weights, return the number of parameters\n",
        "    Inputs: n_input: the number of input units - int\n",
        "          : n_output: the number of output units - int\n",
        "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
        "           : b: a vector bias for the layer - numpy ndarray\n",
        "           : nb_params: the number of parameters  - int\n",
        "    \"\"\"\n",
        "    # START CODE HERE\n",
        "    W=np.random.normal(0,1,size=(n_input, n_output))\n",
        "    b = np.zeros(n_output)\n",
        "    nb_params=n_input+n_output\n",
        "    #nb_params=n_input*n_output + n_output\n",
        "    return W, b, nb_params\n",
        "    # END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DaK_7ui3ld8p"
      },
      "outputs": [],
      "source": [
        "n_training = train_images.shape[0] \n",
        "n_feature = train_images.shape[1] \n",
        "n_labels = 10\n",
        "W, b, nb_params = initLayer(n_feature, n_labels)\n",
        "#print('W shape:', W.shape)\n",
        "#print('b shape:', b.shape)\n",
        "#print('train images shape:', train_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bX7srpjTld8p"
      },
      "outputs": [],
      "source": [
        "def forward(W, b, X):\n",
        "    \"\"\"\n",
        "    Perform the forward propagation\n",
        "    Inputs: W: the weights - numpy ndarray\n",
        "          : b: the bias - numpy ndarray\n",
        "          : X: the batch - numpy ndarray\n",
        "    Outputs: z: outputs - numpy ndarray\n",
        "    \"\"\"\n",
        "    #z = W.T@X + b\n",
        "    #z= np.dot(W.transpose(),X) + b\n",
        "    #z=X @ W + b \n",
        "    z= np.dot(X,W) + b\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z42d6RDld8p"
      },
      "source": [
        "## Computing the output \n",
        "\n",
        "To obtain classification probabilities, we use the softmax function:\n",
        "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
        "\n",
        "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
        "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$\n",
        "what trick can we use to ensure we will not encounter any overflow ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yGztPHoGld8q"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Perform the softmax transformation to the pre-activation values\n",
        "    Inputs: z: the pre-activation values - numpy ndarray\n",
        "    Outputs: out: the activation values - numpy ndarray\n",
        "    \"\"\"\n",
        "    damping = np.max(z)\n",
        "    z = z - damping\n",
        "    out = np.exp(z)/np.sum(np.exp(z))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tiOiK1pld8q"
      },
      "source": [
        "## Making updates\n",
        "\n",
        "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
        "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} - \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
        "\n",
        "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
        "\n",
        "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
        "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
        "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
        "\n",
        "Coordinate by coordinate, we obtain the following update: \n",
        "$$ W_{ij}^{t+1} = W_{ij}^{t} - \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
        "\n",
        "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
        "\n",
        "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
        "\n",
        "We compute the softmax derivative, to obtain:\n",
        "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
        "\n",
        "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
        "\n",
        "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
        "\n",
        "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wsXAoqSmld8r"
      },
      "outputs": [],
      "source": [
        "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
        "    \"\"\"\n",
        "    Perform the update of the parameters\n",
        "    Inputs: eta: the step-size of the gradient descent - float \n",
        "          : W: the weights - ndarray\n",
        "          : b: the bias -  ndarray\n",
        "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
        "          : X: the data -  ndarray\n",
        "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
        "          : weight_decay: the weight decay to be used in updateParams - float\n",
        "    Outputs: W: the weights updated -  ndarray\n",
        "           : b: the bias updated -  ndarray\n",
        "    \"\"\"\n",
        "    grad_w = X.reshape((-1,1)) @ grad.reshape((-1,1)).T\n",
        "    grad_b = grad\n",
        "        \n",
        "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
        "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
        "    return W, b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.reshape((-1,1)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XythUkUIi0mO",
        "outputId": "3e61312b-6781-4324-9b65-622f69f5c179"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47040000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqpWzdnmld8s"
      },
      "source": [
        "The update rule is affected by regularization. We implement two cases: No regularization, or L2 regularization. Use the two possible update rules to implement the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Qi-S3lo_ld8t"
      },
      "outputs": [],
      "source": [
        "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
        "    \"\"\"\n",
        "    Perform the update of the parameters\n",
        "    Inputs: param: the network parameters - ndarray\n",
        "          : grad_param: the updates of the parameters - ndarray\n",
        "          : eta: the step-size of the gradient descent - float\n",
        "          : weight_decay: the weight-decay - float\n",
        "    Outputs: the parameters updated - ndarray\n",
        "    \"\"\"\n",
        "    if regularizer==None:\n",
        "        return param - eta*grad_param\n",
        "    elif regularizer=='L2':\n",
        "        return (1-2*weight_decay)*param - eta*grad_param\n",
        "    else:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBdqTRNHld8t"
      },
      "source": [
        "## Computing the Accuracy\n",
        "\n",
        "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "5l68s07Yld8u"
      },
      "outputs": [],
      "source": [
        "def computeAcc(W, b, X, labels):\n",
        "    \"\"\"\n",
        "    Compute the loss value of the current network on the full batch\n",
        "    Inputs: act_func: the activation function - function\n",
        "          : W: the weights - list of ndarray\n",
        "          : B: the bias - list of ndarray\n",
        "          : X: the batch - ndarray\n",
        "          : labels: the labels corresponding to the batch\n",
        "    Outputs: loss: the negative log-likelihood - float\n",
        "           : accuracy: the ratio of examples that are well-classified - float\n",
        "    \"\"\" \n",
        "    ### Forward propagation\n",
        "    z = forward(W,b,X)\n",
        " \n",
        "    ### Compute the softmax and the prediction\n",
        "    out = softmax(z)\n",
        "    pred = np.argmax(out,axis=1)\n",
        "    true_label=np.argmax(labels,axis=1)\n",
        "    \n",
        "    ###Â Compute the accuracy\n",
        "    accuracy =np.mean(pred==true_label)\n",
        "      \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goCAwyU6ld8u"
      },
      "source": [
        "## Preparing training\n",
        "\n",
        "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each trainin example once. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "h0FZBOv0ld8v"
      },
      "outputs": [],
      "source": [
        "# Optimization\n",
        "eta = 0.01\n",
        "regularizer = 'L2'\n",
        "weight_decay = 0.0001\n",
        "\n",
        "# Training\n",
        "log_interval = 5000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tKixhB5Yld8v",
        "outputId": "ea266604-3a6f-4aa9-ea0c-929dab2d6b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 train_acc=  0.09198333333333333 test_acc=  0.098 eta=  0.01\n",
            "5000 train_acc=  0.8115 test_acc=  0.0982 eta=  0.01\n",
            "10000 train_acc=  0.8461833333333333 test_acc=  0.0985 eta=  0.01\n",
            "15000 train_acc=  0.8296833333333333 test_acc=  0.1077 eta=  0.01\n",
            "20000 train_acc=  0.8419166666666666 test_acc=  0.1124 eta=  0.01\n",
            "25000 train_acc=  0.8201 test_acc=  0.1121 eta=  0.01\n",
            "30000 train_acc=  0.7974666666666667 test_acc=  0.104 eta=  0.01\n",
            "35000 train_acc=  0.7714 test_acc=  0.1028 eta=  0.01\n",
            "40000 train_acc=  0.82195 test_acc=  0.1006 eta=  0.01\n",
            "45000 train_acc=  0.8514166666666667 test_acc=  0.1168 eta=  0.01\n",
            "50000 train_acc=  0.8342666666666667 test_acc=  0.1028 eta=  0.01\n",
            "55000 train_acc=  0.801 test_acc=  0.1071 eta=  0.01\n",
            "Final result:  train_ac= 0.801 test_acc= 0.1071 eta=  0.01\n"
          ]
        }
      ],
      "source": [
        "# Data structures for plotting\n",
        "g_train_acc=[]\n",
        "g_valid_acc=[]\n",
        "\n",
        "#######################\n",
        "### Learning process ##\n",
        "#######################\n",
        "for j in range(n_training):\n",
        "    ### Getting the example\n",
        "    X, y = train_images[j].reshape((1,-1)),y_train[j].reshape((1,-1))\n",
        "    #X, y = train_images[j].reshape((-1,1)),y_train[j].reshape((-1,1))\n",
        "    ### Forward propagation\n",
        "    z = forward(W,b,X)\n",
        "\n",
        "    ### Compute the softmax\n",
        "    out = softmax(z)\n",
        "        \n",
        "    ### Compute the gradient at the top layer\n",
        "    derror = out - y # This is o - y \n",
        "\n",
        "    ### Update the parameters\n",
        "    W, b = update(eta,W,b,derror,X,regularizer,weight_decay)\n",
        "\n",
        "    if j % log_interval == 0:\n",
        "        ### Every log_interval examples, look at the training accuracy\n",
        "        train_accuracy = computeAcc(W, b, train_images, y_train) \n",
        "\n",
        "        ### And the testing accuracy\n",
        "        test_accuracy = computeAcc(W, b, X_test, y_test) \n",
        "\n",
        "        g_train_acc.append(train_accuracy)\n",
        "        g_valid_acc.append(test_accuracy)\n",
        "        result_line = str(int(j)) + \" train_acc=  \" + str(train_accuracy) + \" test_acc=  \" + str(test_accuracy) + \" eta=  \" + str(eta)\n",
        "        print(result_line)\n",
        "\n",
        "g_train_acc.append(train_accuracy)\n",
        "g_valid_acc.append(test_accuracy)\n",
        "result_line = \"Final result: \" + \" train_ac= \" + str(train_accuracy) + \" test_acc= \" + str(test_accuracy) + \" eta=  \" + str(eta)\n",
        "print(result_line)     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cGagPbfld8v"
      },
      "source": [
        "What can you say about the performance of this simple linear classifier ? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have almost the same behaviour even if the train accuracy differ a little bit from the test_accuracy. We have an acceptable performance.The disadvantage of this linear classifier is that, if the data are strongly non-Gaussian, it can perform quite poorly relative to nonlinear classifiers"
      ],
      "metadata": {
        "id": "xoSSE85dSNCB"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Ex1.ipynb.txt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}